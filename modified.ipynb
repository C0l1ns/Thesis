{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler,MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "from core.input_doubling_method import InputDoublingMethod, InputData\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from core.helpers import collect_cluster_center_target_coordinates\n",
    "from core.errors import get_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допоміжні функції"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(x1, x2, reversed=False):\n",
    "    x1_size, x2_size = len(x1), len(x2)\n",
    "    features = np.array(\n",
    "        [\n",
    "            np.concatenate((x2[j][:-1], x1[i][:-1]))\n",
    "            if reversed\n",
    "            else np.concatenate((x1[i][:-1], x2[j][:-1]))\n",
    "            for i in range(x1_size)\n",
    "            for j in range(x2_size)\n",
    "        ]\n",
    "    )\n",
    "    labels = np.array(\n",
    "        [\n",
    "            (x2[j][-1] - x1[i][-1]) if reversed else (x1[i][-1] - x2[j][-1])\n",
    "            for i in range(x1_size)\n",
    "            for j in range(x2_size)\n",
    "        ]\n",
    "    )\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_yn(z, y_sum, N):\n",
    "    return np.array([(y_sum + sum(z[i : i + N])) / N for i in range(0, len(z), N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_centers(k, data, best_labels):\n",
    "\n",
    "    cluster, count = np.unique(best_labels, return_counts=True)\n",
    "    clusters_y = {i: 0 for i in range(k)}\n",
    "    for index, cluster in enumerate(best_labels):\n",
    "        clusters_y[cluster] += data[index]\n",
    "\n",
    "    for k in clusters_y.keys():\n",
    "        clusters_y[k] /= count[k]\n",
    "\n",
    "    return clusters_y\n",
    "\n",
    "\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "\n",
    "def find_closest_cluster(vector, cluster_centers):\n",
    "    min_distance = float(\"inf\")\n",
    "    min_distance_index = float(\"inf\")\n",
    "\n",
    "    for index, cluster in enumerate(cluster_centers):\n",
    "        distance = euclidean_distance(vector, cluster)\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            min_distance_index = index\n",
    "\n",
    "    return min_distance_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вичитка даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_random_state = 42\n",
    "kmeans_random_state_out = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on dataset:  franke_function\n"
     ]
    }
   ],
   "source": [
    "# train_dataset_path = \"./datasets/FractureTrain.txt\"\n",
    "# test_dataset_path = \"./datasets/FractureTest.txt\"\n",
    "\n",
    "# train_data = np.loadtxt(train_dataset_path, delimiter=\",\")\n",
    "# test_data = np.loadtxt(test_dataset_path, delimiter=\",\")\n",
    "# X_train, y_train = train_data[:, :-1], train_data[:, -1:]\n",
    "# X_test, y_test = test_data[:, :-1], test_data[:, -1:]\n",
    "\n",
    "# keep in mind that check for variable in locals mean that it will require cleaning state all the time\n",
    "if not \"dataset\" in locals():\n",
    "    dataset = \"franke_function\"\n",
    "print(\"Running on dataset: \", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    pd.read_csv(f\"./datasets/{dataset}.csv\")\n",
    "    .to_numpy()\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[:, :-1], data[:, -1:], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_data = np.concatenate((X_train, y_train), axis=1)\n",
    "test_data = np.concatenate((X_test, y_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = sum(\n",
    "    [a[0] for a in y_train]\n",
    ")  # просумована таргет колонка(вона тут остання) 20.5, 13.3, 19.6, 24.4 ...\n",
    "N = len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Базові опції"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel=\"rbf\", gamma=\"scale\", coef0=0.0, epsilon=0.001, max_iter=-1)\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "rfg = RandomForestRegressor(random_state=42, max_depth=5)\n",
    "output_errors_train, output_errors_test  = {},{}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Без виходу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of clusters:  2\n"
     ]
    }
   ],
   "source": [
    "N_CLUSTERS = 2\n",
    "if not 'N_CLUSTERS' in locals():\n",
    "    N_CLUSTERS = 7\n",
    "print(\"Num of clusters: \", N_CLUSTERS)\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=kmeans_random_state, n_init=\"auto\").fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_y = calculate_cluster_centers(\n",
    "    N_CLUSTERS, y_train.flatten(), kmeans.labels_\n",
    ")\n",
    "# cluster_centers = np.hstack((kmeans_train.cluster_centers_,np.array(list(cluster_centers_y.values())).reshape(-1,1)))\n",
    "new_y_train = [cluster_centers_y[label] for label in kmeans.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [\n",
    "    find_closest_cluster(vector, kmeans.cluster_centers_) for vector in X_test\n",
    "]\n",
    "new_y_test = [cluster_centers_y[label] for label in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_train_data = np.concatenate(\n",
    "    (\n",
    "        train_data[:, :-1],\n",
    "        np.array(new_y_train).reshape(-1, 1),\n",
    "        train_data[:, -1][:, None],\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "enriched_test_data = np.concatenate(\n",
    "    (test_data[:, :-1], np.array(new_y_test).reshape(-1, 1), test_data[:, -1][:, None]),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = sum(\n",
    "    [a[-1] for a in enriched_train_data]\n",
    ")  # просумована таргет колонка(вона тут остання) 20.5, 13.3, 19.6, 24.4 ...\n",
    "N = len(enriched_train_data)\n",
    "y_target_test = np.array(\n",
    "    [a[-1] for a in enriched_test_data]\n",
    ")  # таргет колонка тесту перетворена у вектор\n",
    "y_target_train = np.array(\n",
    "    [a[-1] for a in enriched_train_data]\n",
    ")  # таргет колонка трейну перетворена у вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = preprocess_features(\n",
    "    enriched_train_data, enriched_train_data\n",
    ")\n",
    "test_features, test_labels = preprocess_features(\n",
    "    enriched_test_data, enriched_train_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(train_features)\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результати без виходу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.786806583404541 seconds ---\n"
     ]
    }
   ],
   "source": [
    "idm = InputDoublingMethod(y_sum=y_sum, N=N)\n",
    "input_data = InputData(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_features,\n",
    "    test_labels,\n",
    "    y_target_train,\n",
    "    y_target_test,\n",
    ")\n",
    "# ---------------------------------------------------------------------------------\n",
    "yn_train, yn_test = idm.apply(svr, input_data)\n",
    "train_erros = [value for name, value in get_errors(y_target_train, yn_train)]\n",
    "test_errors = [value for name, value in get_errors(y_target_test, yn_test)]\n",
    "\n",
    "output_errors_train[f\"без виходу SVR - {N_CLUSTERS}\"] = train_erros\n",
    "output_errors_test[f\"без виходу SVR - {N_CLUSTERS}\"] = test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# З виходом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_out = KMeans(n_clusters=N_CLUSTERS, random_state=kmeans_random_state_out, n_init=\"auto\").fit(\n",
    "    train_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_without_y = kmeans_out.cluster_centers_[:, :-1]\n",
    "test_labels_out = [\n",
    "    find_closest_cluster(vector, cluster_centers_without_y) for vector in X_test\n",
    "]\n",
    "\n",
    "new_y_train_out = collect_cluster_center_target_coordinates(\n",
    "    kmeans_out.cluster_centers_, kmeans_out.labels_\n",
    ")\n",
    "new_y_test_out = [\n",
    "    kmeans_out.cluster_centers_[label][kmeans_out.cluster_centers_.shape[1] - 1]\n",
    "    for label in test_labels_out\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_train_data_out = np.concatenate(\n",
    "    (\n",
    "        train_data[:, :-1],\n",
    "        np.array(new_y_train_out).reshape(-1, 1),\n",
    "        train_data[:, -1][:, None],\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "enriched_test_data_out = np.concatenate(\n",
    "    (\n",
    "        test_data[:, :-1],\n",
    "        np.array(new_y_test_out).reshape(-1, 1),\n",
    "        test_data[:, -1][:, None],\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_out, train_labels_out = preprocess_features(\n",
    "    enriched_train_data_out, enriched_train_data_out\n",
    ")  # додаємо в кінець одного вектора інший вектор(процедура аугментації)\n",
    "# train_labels це наші z_1,z_2,z_3, z_4\n",
    "# робиться те саме що і в минулому випадку, але навпаки перший вектор йде в кінець а наступні на початок\n",
    "# train_labels2 точно такі самі як і train_labels тільки з іншим знаком\n",
    "test_features_out, test_labels_out = preprocess_features(\n",
    "    enriched_test_data_out, enriched_train_data_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(train_features_out)\n",
    "train_features_out = scaler.transform(train_features_out)\n",
    "test_features_out = scaler.transform(test_features_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результати з виходом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.516742467880249 seconds ---\n"
     ]
    }
   ],
   "source": [
    "idm_out = InputDoublingMethod(y_sum=y_sum, N=N)\n",
    "input_data_out = InputData(\n",
    "    train_features_out,\n",
    "    train_labels_out,\n",
    "    test_features_out,\n",
    "    test_labels_out,\n",
    "    y_target_train,\n",
    "    y_target_test,\n",
    ")\n",
    "# ---------------------------------------------------------------------------------\n",
    "yn_train, yn_test = idm_out.apply(svr, input_data_out)\n",
    "train_erros = [value for name, value in get_errors(y_target_train, yn_train)]\n",
    "test_errors = [value for name, value in get_errors(y_target_test, yn_test)]\n",
    "\n",
    "output_errors_train[f\"З виходом SVR - {N_CLUSTERS}\"] = train_erros\n",
    "output_errors_test[f\"З виходом SVR - {N_CLUSTERS}\"] = test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputDoublingMethod.save_errors_to_csv(output_errors_train, f\"results/{dataset}_errors_train_n{N_CLUSTERS}.csv\")\n",
    "InputDoublingMethod.save_errors_to_csv(output_errors_test, f\"results/{dataset}_errors_test_n{N_CLUSTERS}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
